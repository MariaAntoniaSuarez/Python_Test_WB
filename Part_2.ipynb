{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5pzUUBfFB9R",
        "outputId": "661bbc3d-eb19-499c-abd0-8c926527e7f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.cluster import DBSCAN\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Load the embeddings model and the NLP model\n",
        "model = SentenceTransformer('paraphrase-mpnet-base-v2')  # Pre-trained model of embeddings\n",
        "nlp = spacy.load('en_core_web_sm')  # Model NLP\n",
        "\n",
        "# Customized list of irrelevant words in company names\n",
        "EXTRA_STOP_WORDS = {\"inc\", \"corp\", \"co\", \"ltd\", \"group\", \"corporation\", \"limited\", \"company\", \"mall\", \"road\", \"street\"}\n",
        "\n",
        "# Function to clean up company names using spaCy and eliminate stop words\n",
        "def clean_firm_name(name):\n",
        "    doc = nlp(name.lower())\n",
        "    cleaned_name = ' '.join(\n",
        "        [token.lemma_ for token in doc if not token.is_stop and token.is_alpha and token.lemma_ not in EXTRA_STOP_WORDS]\n",
        "    )\n",
        "    return cleaned_name\n",
        "\n",
        "# Function to calculate embeddings of company names in batches\n",
        "def get_embeddings(names):\n",
        "    batch_size = 90  # Sets this value according to the available memory\n",
        "    embeddings = []\n",
        "    for i in range(0, len(names), batch_size):\n",
        "        batch = names[i:i + batch_size]\n",
        "        embeddings.extend(model.encode(batch, convert_to_tensor=True))\n",
        "    return np.array(embeddings)\n",
        "\n",
        "\n",
        "# Function for grouping similar names using DBSCAN\n",
        "def group_similar_names_with_dbscan(embeddings, names, eps=0.3, min_samples=3):\n",
        "    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine').fit(embeddings)\n",
        "    grouped_names = defaultdict(list)\n",
        "\n",
        "    for idx, label in enumerate(clustering.labels_):\n",
        "        grouped_names[label].append(names[idx])\n",
        "\n",
        "    # Filtrar outliers (label -1 indica ruido)\n",
        "    grouped_names = {k: v for k, v in grouped_names.items() if k != -1}\n",
        "    return grouped_names\n",
        "\n",
        "# Function for filtering names using cosine similarity after DBSCAN\n",
        "def filter_similar_names(grouped_names, embeddings, names, threshold=0.8):\n",
        "    refined_groups = defaultdict(list)\n",
        "\n",
        "    for label, group_names in grouped_names.items():\n",
        "        group_indices = [names.index(name) for name in group_names]\n",
        "        name_vectors = [embeddings[idx] for idx in group_indices]\n",
        "        similarity_matrix = cosine_similarity(name_vectors)\n",
        "\n",
        "        for i, name1 in enumerate(group_names):\n",
        "            for j, name2 in enumerate(group_names):\n",
        "                if i != j and similarity_matrix[i][j] >= threshold:\n",
        "                    refined_groups[label].append(name1)\n",
        "                    refined_groups[label].append(name2)\n",
        "\n",
        "        # Eliminate duplicates within each group and keep only names with sufficient similarity\n",
        "        refined_groups[label] = list(set(refined_groups[label]))\n",
        "\n",
        "    # Filter out groups that were left empty\n",
        "    refined_groups = {k: v for k, v in refined_groups.items() if len(v) > 1}\n",
        "    return refined_groups\n",
        "\n",
        "# Function to process names in parallel and calculate embeddings\n",
        "def parallel_process_firm_names(group, text_column, max_workers=None):\n",
        "    if max_workers is None:\n",
        "        max_workers = os.cpu_count() * 2\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        tqdm.pandas(desc=\"Cleaning Firm Names\")\n",
        "        cleaned_names = list(executor.map(clean_firm_name, group[text_column].tolist()))\n",
        "\n",
        "    # Get embeddings of the cleaned names in batches\n",
        "    embeddings = get_embeddings(cleaned_names)\n",
        "\n",
        "    # Grouping similar names using DBSCAN\n",
        "    grouped_names = group_similar_names_with_dbscan(embeddings, cleaned_names, eps=0.7, min_samples=2)\n",
        "\n",
        "    # Filtering names using cosine similarity to improve the quality of the groups\n",
        "    refined_grouped_names = filter_similar_names(grouped_names, embeddings, cleaned_names, threshold=0.7)\n",
        "\n",
        "    return refined_grouped_names, pd.DataFrame({text_column: cleaned_names, 'cleaned_name': cleaned_names})\n",
        "\n",
        "# Function for assigning unique IDs and choosing the best name\n",
        "def assign_unique_ids_and_best_names(grouped_names, original_group, text_column, country_name):\n",
        "    cleaned_data = []\n",
        "\n",
        "    for group, names in grouped_names.items():\n",
        "        best_name = max(names, key=len)  # Choosing the longest name as the best name\n",
        "        original_names = original_group[original_group['cleaned_name'].isin(names)][text_column].tolist()\n",
        "\n",
        "        cleaned_data.append({\n",
        "            'cleaned_ID': id(best_name),\n",
        "            'cleaned_name': best_name,\n",
        "            'original_firm_names': original_names,\n",
        "            'country': country_name\n",
        "        })\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "\n",
        "# Main function to clean, group and assign IDs by group with parallelization by country using ThreadPoolExecutor\n",
        "def clean_and_group_by_country(df, text_column, country_column, max_workers=None):\n",
        "    all_cleaned_data = []\n",
        "\n",
        "    # Group by country\n",
        "    grouped_df = df.groupby(country_column)\n",
        "\n",
        "    # Create a progress bar with the total number of countries to process\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {\n",
        "            executor.submit(\n",
        "                parallel_process_firm_names, group, text_column, max_workers\n",
        "            ): (country, group) for country, group in grouped_df\n",
        "        }\n",
        "\n",
        "        # Configure tqdm manually\n",
        "        with tqdm(total=len(futures), desc=\"Processing Countries\") as pbar:\n",
        "            for future in as_completed(futures):\n",
        "                country, group = futures[future]\n",
        "                try:\n",
        "                    grouped_names, original_group = future.result()\n",
        "                    cleaned_data = assign_unique_ids_and_best_names(grouped_names, original_group, text_column, country)\n",
        "                    all_cleaned_data.extend(cleaned_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing country {country}: {e}\")\n",
        "\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "\n",
        "    # Create a new DataFrame with the results.\n",
        "    cleaned_df = pd.DataFrame(all_cleaned_data)\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Load data up to 2020 and new data for 2021\n",
        "df_old = pd.read_csv('ForeignNames_2019_2020.csv')\n",
        "df_new = pd.read_csv('ForeignNames_2021.csv')\n",
        "# Add a 'new' column to identify the data for 2021\n",
        "df_old['new'] = 0\n",
        "df_new['new'] = 1\n",
        "# Join both DataFramess\n",
        "df = pd.concat([df_old, df_new], ignore_index=True)\n",
        "df['cleaned_ID'] = df['foreign']\n",
        "df['cleaned_ID'] = df['cleaned_ID'].str.lower()\n",
        "df['foreign'] = df['foreign'].str.lower()\n",
        "df = df.dropna()\n",
        "df= df.sample(n=60000)\n",
        "df= df.groupby(['cleaned_ID','foreign', 'foreigncountry_cleaned', 'shpmtyear']).count().reset_index()\n",
        "processed_df = clean_and_group_by_country(df,'foreign', 'foreigncountry_cleaned', max_workers=40)\n",
        "# Merge again with the original data to obtain the original company name and the column 'new'\n",
        "processed_df = processed_df.merge(df[['foreign', 'cleaned_ID', 'new']], on='cleaned_ID', how='left')\n",
        "# Get the 'old_name' where 'new' is 0, i.e. from 2019-2020 data\n",
        "processed_df['old_name'] = processed_df.apply(lambda row: row['foreign'] if row['new'] == 0 else None, axis=1)\n",
        "# Rename column 'foreign' to 'cleaned_name' for clarity\n",
        "processed_df.rename(columns={'foreign': 'cleaned_name'}, inplace=True)\n",
        "# Explode\n",
        "df_explotado = processed_df.explode('original_firm_names')\n",
        "# Upload Country_Name_ISO3.csv\n",
        "iso_df = pd.read_csv('Country_Name_ISO3.csv')\n",
        "# Ensure that the columns have the same data type.\n",
        "df_explotado['country'] = df_explotado['country'].astype(str)\n",
        "iso_df['country_name'] = iso_df['country_name'].astype(str)\n",
        "# Perform the merge between the two databases using the corresponding columns\n",
        "df_1 = df_explotado.merge(iso_df[['country_name', 'country_iso3']],\n",
        "                     how='left',\n",
        "                     left_on='country',\n",
        "                     right_on='country_name')\n",
        "# Export the resulting DataFrame to a CSV file with the specified name\n",
        "df_changed.to_csv('outputfile_Maria_2.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weLuhXuAJ9G7",
        "outputId": "dfe412d1-4009-48ea-f165-b59a42ad5a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Countries:  90%|████████▉ | 193/215 [17:58<10:50, 29.57s/it]"
          ]
        }
      ]
    }
  ]
}